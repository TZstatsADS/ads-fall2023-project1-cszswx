# Load required libraries
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(scales)
library(wordcloud2)
library(gridExtra)
library(ggplot2)
library(ngram)
library(shiny)

# Define a function to preprocess text
preprocess_text <- function(text) {
  # Text preprocessing steps
  corpus <- VCorpus(VectorSource(text)) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeNumbers) %>%
    tm_map(removeWords, character(0)) %>%
    tm_map(stripWhitespace)
  
  stemmed <- tm_map(corpus, stemDocument) %>%
    tidy() %>%
    select(text)
  
  dict <- tidy(corpus) %>%
    select(text) %>%
    unnest_tokens(dictionary, text)
  
  data("stop_words")
  
  word <- c("happy", "ago", "yesterday", "lot", "today", "months", "month",
            "happier", "happiest", "last", "week", "past")
  
  stop_words <- stop_words %>%
    bind_rows(mutate(tibble(word), lexicon = "updated"))
  
  completed <- stemmed %>%
    mutate(id = row_number()) %>%
    unnest_tokens(stems, text) %>%
    bind_cols(dict) %>%
    anti_join(stop_words, by = c("dictionary" = "word"))
  
  completed <- completed %>%
    group_by(stems) %>%
    count(dictionary) %>%
    mutate(word = dictionary[which.max(n)]) %>%
    ungroup() %>%
    select(stems, word) %>%
    distinct() %>%
    right_join(completed) %>%
    select(-stems)
  
  completed <- completed %>%
    group_by(id) %>%
    summarise(text = str_c(word, collapse = " ")) %>%
    ungroup()
  
  return(completed)
}

# Define a function to read and process data
read_and_process_data <- function(data_path, dict_path, stop_words_path) {
  # Read data
  hm_data <- read_csv(data_path)
  
  # Preprocess text
  hm_data <- hm_data %>%
    mutate(cleaned_text = preprocess_text(cleaned_hm))
  
  # ... (other data processing steps)
  
  return(hm_data)
}

# Main code
hm_data <- read_and_process_data("/Users/jaysun/Downloads/HappyDB-master/happydb/data/cleaned_hm.csv", 
                                 "dictionary.csv", "stopwords.csv")

# Analyze word frequency and bigrams
bag_of_words <-  hm_data %>%
  unnest_tokens(word, cleaned_text)

word_count <- bag_of_words %>%
  count(word, sort = TRUE)

hm_bigrams <- hm_data %>%
  filter(count != 1) %>%
  unnest_tokens(bigram, cleaned_text, token = "ngrams", n = 2)

bigram_counts <- hm_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  count(word1, word2, sort = TRUE)

# Create visualizations
wc <- word_count %>% filter(n > 3100)
ggplot(data = wc, aes(x = word, y = n)) + geom_bar(stat = "identity", fill = "green")

# ... (continue with your ggplot visualizations)

# Save visualizations as PDF (optional)
ggsave("word_count.pdf", plot = wc, width = 8, height = 4)

# Combine visualizations
grid_arrange_plot <- grid.arrange(pp, pr, pg, pm, ncol = 2, nrow = 2)

# Analyze word frequency by age group
processed <- bag_of_words %>%
  select(age, word) %>%
  mutate(age_group = cut(age, seq(0, 100, by = 5), right = FALSE))

wc <- processed %>%
  group_by(age_group, word) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

top_words <- wc %>%
  group_by(age_group) %>%
  top_n(3, count)

filtered_top_words <- top_words %>%
  group_by(age_group) %>%
  filter(sum(count) > 0, !is.na(age_group))

p <- ggplot(filtered_top_words, aes(x = age_group, y = count, fill = word)) +
  geom_bar(stat = "identity") +
  facet_wrap(~age_group, scales = "free_y") +
  labs(title = "Top 3 Words Used in Happy Moments by Age Group (grouped by 5 years)",
       x = "Age Group",
       y = "Word Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Save the larger plot to a file
ggsave("happy_words_plot.png", plot = p, width = 24, height = 12)

# Output the combined visualization and analysis
pdf("combined_visualization.pdf", width = 16, height = 8)
print(grid_arrange_plot)
dev.off()
 